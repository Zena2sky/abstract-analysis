{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어: 인문학, 사회과학\n",
        "\n",
        "선정된 과잉 단어들(excess_words_for_labeling_final.xlxs)이 실제 초록에서 연도별로 얼마나 사용되는지 추이 파악하는 코드\n",
        "\n",
        "- 측정 지표\n",
        "1. excess_count 총 출현 횟수(중복 포함한 전체 사용량)\n",
        "2. excess_unique 고유 단어 개수 (몇 종류의 과잉 단어들을 쓰는가?)\n",
        "- found_words에 같은 과잉 단어가 여러 번 나오더라도 종류로는 1개로 침. (예시: \"시사한다\"가 10번 나와도 unique_count에서는 1번만 카운트 되는 것임.)\n",
        "3. excess_ratio 과잉 단어,어절 수\n",
        "- 예를 들어, 초록이 100어절에 과잉 단어가 4번 나온 거면 ratio는 4퍼센트인 것.\n",
        "\n",
        "과잉 단어 N개 이상 사용한 초록이 몇 퍼센트인가? (현재 1, 3, 5, 10, 15, 20개를 기준으로 함.)\n",
        "연도별 평균, 중앙값, 분포 변화를 보고 있음.\n",
        "\n"
      ],
      "metadata": {
        "id": "q2-fS0FCkzGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한글 폰트 설정 (Windows의 경우)\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 1. 과잉 단어 파일 경로\n",
        "# 필요한 파일: (H 혹은 SS)_excess_words_top_combined_for_labeling_final.xlsx\n",
        "EXCESS_WORDS_FILE = \"/content/SS_excess_words_top_combined_for_labeling_final.xlsx\"\n",
        "\n",
        "# 2. 원본 초록 데이터 파일 경로 (전처리된 CSV/Excel)\n",
        "# 필요한 파일: (H 혹은 SS)_kor_abstract_clean.csv\n",
        "ABSTRACTS_FILE = \"/content/SS_kor_abstract_clean.csv\"\n",
        "\n",
        "# 3. 출력된 폴더\n",
        "OUTPUT_DIR = Path(\"/content/excess_analysis\")\n",
        "\n",
        "# 4. 분석 기준 (과잉 단어 N개 이상)\n",
        "THRESHOLDS = [1, 3, 5, 10, 15, 20]\n",
        "\n",
        "# 5. 분야명 (그래프 제목용)\n",
        "FIELD_NAME = \"Humanities\"  # 또는 \"Social Sciences\"\n",
        "\n",
        "\n",
        "def load_excess_words(filepath):\n",
        "    \"\"\"과잉 단어 목록 로드\"\"\"\n",
        "    df = pd.read_excel(filepath)\n",
        "    words = set(df['word'].tolist())\n",
        "    print(f\"과잉 단어 {len(words)}개 로드 완료\")\n",
        "    return words, df\n",
        "\n",
        "\n",
        "def load_abstracts(filepath, encoding='utf-8-sig'):\n",
        "    \"\"\"초록 데이터 로드\"\"\"\n",
        "    if filepath.endswith('.xlsx') or filepath.endswith('.xls'):\n",
        "        df = pd.read_excel(filepath)\n",
        "    else:\n",
        "        df = pd.read_csv(filepath, encoding=encoding)\n",
        "    print(f\"초록 데이터 {len(df)}건 로드 완료\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def count_excess_in_text(text, excess_words):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return 0, 0, []\n",
        "\n",
        "    found_words = [] # 발견된 단어들\n",
        "    for word in excess_words:\n",
        "        cnt = text.count(word) # 총 출현 횟수(같은 단어가 여러 번 나오면 다 셈.)\n",
        "        if cnt > 0:\n",
        "            found_words.extend([word] * cnt)\n",
        "\n",
        "    count = len(found_words)\n",
        "    unique_count = len(set(found_words)) # 고유 과잉 단어 개수\n",
        "\n",
        "    return count, unique_count, list(set(found_words))\n",
        "\n",
        "\n",
        "# 모든 초록 돌면서 각각에 대하여 과잉 단어 개수, 텍스트 길이(어절 수), 과잉 비율=과잉 단어 수/전체 어절 수\n",
        "def analyze_all_abstracts(df, excess_words, year_col='발행년도', abstract_col='초록'):\n",
        "\n",
        "\n",
        "    results = []\n",
        "    total = len(df)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if idx % 10000 == 0:\n",
        "            print(f\"  진행: {idx:,} / {total:,} ({idx/total*100:.1f}%)\")\n",
        "\n",
        "        year = row[year_col]\n",
        "        text = row[abstract_col]\n",
        "\n",
        "        count, unique_count, found = count_excess_in_text(text, excess_words)\n",
        "\n",
        "        # 텍스트 길이 (대략적인 어절 수)\n",
        "        text_len = len(str(text).split()) if pd.notna(text) else 0\n",
        "        ratio = count / text_len if text_len > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            'year': year,\n",
        "            'excess_count': count,\n",
        "            'excess_unique': unique_count,\n",
        "            'excess_ratio': ratio,\n",
        "            'text_length': text_len\n",
        "        })\n",
        "\n",
        "    print(f\"  완료: {total:,}건 분석\")\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def calculate_threshold_stats(analysis_df, thresholds):\n",
        "    \"\"\"방법 2: N개 이상 과잉 단어 사용한 초록 비율 계산\"\"\"\n",
        "    years = sorted(analysis_df['year'].unique())\n",
        "\n",
        "    stats = []\n",
        "    for year in years:\n",
        "        year_data = analysis_df[analysis_df['year'] == year]\n",
        "        total = len(year_data)\n",
        "\n",
        "        row = {'year': year, 'total_abstracts': total}\n",
        "\n",
        "        for n in thresholds:\n",
        "            count = (year_data['excess_unique'] >= n).sum()\n",
        "            ratio = count / total * 100 if total > 0 else 0\n",
        "            row[f'n>={n}_count'] = count\n",
        "            row[f'n>={n}_ratio'] = ratio\n",
        "\n",
        "        stats.append(row)\n",
        "\n",
        "    return pd.DataFrame(stats)\n",
        "\n",
        "\n",
        "def plot_threshold_trends(stats_df, thresholds, output_path, field_name=\"\"):\n",
        "    \"\"\"방법 2 시각화: N개 이상 사용 비율 추이\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    years = stats_df['year'].values\n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(thresholds)))\n",
        "\n",
        "    for i, n in enumerate(thresholds):\n",
        "        col = f'n>={n}_ratio'\n",
        "        ax.plot(years, stats_df[col], marker='o', label=f'{n}+ words',\n",
        "                color=colors[i], linewidth=2, markersize=5)\n",
        "\n",
        "    ax.set_xlabel('Year', fontsize=12)\n",
        "    ax.set_ylabel('Percentage of Abstracts (%)', fontsize=12)\n",
        "    ax.set_title(f'[{field_name}] Abstracts Using N+ Excess Words Over Time', fontsize=14)\n",
        "    ax.legend(loc='upper left', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xticks(years[::2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"저장: {output_path}\")\n",
        "\n",
        "\n",
        "def plot_score_distribution(analysis_df, output_path, field_name=\"\"):\n",
        "    \"\"\"방법 3 시각화: 연도별 과잉 점수 분포 (박스플롯)\"\"\"\n",
        "    years = sorted(analysis_df['year'].unique())\n",
        "    recent_years = years[-10:] if len(years) > 10 else years\n",
        "    recent_data = analysis_df[analysis_df['year'].isin(recent_years)]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # (1) 고유 과잉 단어 개수 분포\n",
        "    ax1 = axes[0]\n",
        "    data_unique = [recent_data[recent_data['year'] == y]['excess_unique'].values\n",
        "                   for y in recent_years]\n",
        "    bp1 = ax1.boxplot(data_unique, labels=recent_years, patch_artist=True)\n",
        "    for patch in bp1['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "    ax1.set_xlabel('Year', fontsize=11)\n",
        "    ax1.set_ylabel('Unique Excess Words per Abstract', fontsize=11)\n",
        "    ax1.set_title(f'[{field_name}] Distribution of Excess Word Count', fontsize=12)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # (2) 과잉 단어 비율 분포\n",
        "    ax2 = axes[1]\n",
        "    data_ratio = [recent_data[recent_data['year'] == y]['excess_ratio'].values * 100\n",
        "                  for y in recent_years]\n",
        "    bp2 = ax2.boxplot(data_ratio, labels=recent_years, patch_artist=True)\n",
        "    for patch in bp2['boxes']:\n",
        "        patch.set_facecolor('lightcoral')\n",
        "    ax2.set_xlabel('Year', fontsize=11)\n",
        "    ax2.set_ylabel('Excess Word Ratio (%)', fontsize=11)\n",
        "    ax2.set_title(f'[{field_name}] Distribution of Excess Word Ratio', fontsize=12)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"저장: {output_path}\")\n",
        "\n",
        "\n",
        "def plot_yearly_mean_trend(analysis_df, output_path, field_name=\"\"):\n",
        "    \"\"\"연도별 평균 과잉 단어 사용량 추이\"\"\"\n",
        "    yearly_stats = analysis_df.groupby('year').agg({\n",
        "        'excess_count': ['mean', 'std', 'median'],\n",
        "        'excess_unique': ['mean', 'std', 'median'],\n",
        "        'excess_ratio': ['mean', 'std', 'median']\n",
        "    }).reset_index()\n",
        "\n",
        "    yearly_stats.columns = ['year',\n",
        "                            'count_mean', 'count_std', 'count_median',\n",
        "                            'unique_mean', 'unique_std', 'unique_median',\n",
        "                            'ratio_mean', 'ratio_std', 'ratio_median']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    years = yearly_stats['year'].values\n",
        "\n",
        "    # (1) 평균 고유 과잉 단어 개수\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(years, yearly_stats['unique_mean'], 'b-o', linewidth=2,\n",
        "             markersize=6, label='Mean')\n",
        "    ax1.plot(years, yearly_stats['unique_median'], 'b--s', linewidth=1.5,\n",
        "             markersize=4, alpha=0.7, label='Median')\n",
        "\n",
        "    ax1.set_xlabel('Year', fontsize=11)\n",
        "    ax1.set_ylabel('Unique Excess Words', fontsize=11)\n",
        "    ax1.set_title(f'[{field_name}] Mean Excess Word Count per Abstract', fontsize=12)\n",
        "    ax1.legend(fontsize=9)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    n_samples = analysis_df.groupby('year').size().values\n",
        "    # 음영 부분\n",
        "    ci_unique = 1.96 * yearly_stats['unique_std'] / np.sqrt(n_samples)\n",
        "    ax1.fill_between(years,\n",
        "                 yearly_stats['unique_mean'] - ci_unique,\n",
        "                 yearly_stats['unique_mean'] + ci_unique,\n",
        "                 alpha=0.2)\n",
        "    ax1.set_xticks(years[::2])\n",
        "\n",
        "    # (2) 평균 과잉 단어 비율\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(years, yearly_stats['ratio_mean'] * 100, 'r-o', linewidth=2,\n",
        "             markersize=6, label='Mean')\n",
        "    ax2.plot(years, yearly_stats['ratio_median'] * 100, 'r--s', linewidth=1.5,\n",
        "             markersize=4, alpha=0.7, label='Median')\n",
        "\n",
        "    ax2.set_xlabel('Year', fontsize=11)\n",
        "    ax2.set_ylabel('Excess Word Ratio (%)', fontsize=11)\n",
        "    ax2.set_title(f'[{field_name}] Mean Excess Word Ratio per Abstract', fontsize=12)\n",
        "    ax2.legend(fontsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    ci_ratio = 1.96 * yearly_stats['ratio_std'] / np.sqrt(n_samples)\n",
        "    # 음영 부분\n",
        "    ax2.fill_between(years,\n",
        "                     (yearly_stats['ratio_mean'] - yearly_stats['ratio_std']) * 100,\n",
        "                     (yearly_stats['ratio_mean'] + yearly_stats['ratio_std']) * 100,\n",
        "                     alpha=0.2, color='red')\n",
        "     ax2.set_xticks(years[::2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"저장: {output_path}\")\n",
        "\n",
        "    return yearly_stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"과잉 단어 사용 추이 분석\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 출력 폴더 생성\n",
        "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1. 과잉 단어 로드\n",
        "    print(\"\\n[1] 과잉 단어 로드 중...\")\n",
        "    excess_words, excess_df = load_excess_words(EXCESS_WORDS_FILE)\n",
        "\n",
        "    # 2. 초록 데이터 로드\n",
        "    print(\"\\n[2] 초록 데이터 로드 중...\")\n",
        "    abstracts_df = load_abstracts(ABSTRACTS_FILE)\n",
        "\n",
        "    # 3. 분석 실행\n",
        "    print(\"\\n[3] 초록별 과잉 단어 분석...\")\n",
        "    analysis_df = analyze_all_abstracts(abstracts_df, excess_words)\n",
        "\n",
        "    # 분석 결과 저장\n",
        "    analysis_df.to_csv(OUTPUT_DIR / 'abstract_excess_analysis.csv',\n",
        "                       index=False, encoding='utf-8-sig')\n",
        "    print(f\"  → 저장: {OUTPUT_DIR / 'abstract_excess_analysis.csv'}\")\n",
        "\n",
        "    # 4. 방법 2: N개 이상 사용 비율\n",
        "    print(\"\\n[4] 임계값별 비율 계산...\")\n",
        "    stats_df = calculate_threshold_stats(analysis_df, THRESHOLDS)\n",
        "    stats_df.to_csv(OUTPUT_DIR / 'threshold_stats.csv',\n",
        "                    index=False, encoding='utf-8-sig')\n",
        "    print(stats_df.to_string())\n",
        "\n",
        "    # 5. 시각화\n",
        "    print(\"\\n[5] 시각화...\")\n",
        "    plot_threshold_trends(stats_df, THRESHOLDS,\n",
        "                          OUTPUT_DIR / 'threshold_trends.png', FIELD_NAME)\n",
        "    plot_score_distribution(analysis_df,\n",
        "                            OUTPUT_DIR / 'score_distribution.png', FIELD_NAME)\n",
        "    yearly_stats = plot_yearly_mean_trend(analysis_df,\n",
        "                                          OUTPUT_DIR / 'yearly_mean_trend.png', FIELD_NAME)\n",
        "\n",
        "    # 연도별 통계 저장\n",
        "    yearly_stats.to_csv(OUTPUT_DIR / 'yearly_stats.csv',\n",
        "                        index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 분석 완료!\")\n",
        "    print(f\"   결과 저장 위치: {OUTPUT_DIR}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 요약 통계 출력\n",
        "    print(\"\\n[요약]\")\n",
        "    print(f\"- 전체 초록 수: {len(analysis_df):,}\")\n",
        "    print(f\"- 과잉 단어 1개 이상 사용한 초록: {(analysis_df['excess_unique'] >= 1).sum():,} \"\n",
        "          f\"({(analysis_df['excess_unique'] >= 1).mean()*100:.1f}%)\")\n",
        "    print(f\"- 과잉 단어 5개 이상 사용한 초록: {(analysis_df['excess_unique'] >= 5).sum():,} \"\n",
        "          f\"({(analysis_df['excess_unique'] >= 5).mean()*100:.1f}%)\")\n",
        "    print(f\"- 평균 과잉 단어 개수: {analysis_df['excess_unique'].mean():.2f}\")\n",
        "\n",
        "    # 2024년 vs 2020년 비교\n",
        "    if 2024 in analysis_df['year'].values and 2020 in analysis_df['year'].values:\n",
        "        y2020 = analysis_df[analysis_df['year'] == 2020]['excess_unique'].mean()\n",
        "        y2024 = analysis_df[analysis_df['year'] == 2024]['excess_unique'].mean()\n",
        "        print(f\"\\n- 2020년 평균: {y2020:.2f} → 2024년 평균: {y2024:.2f} \"\n",
        "              f\"(변화: {(y2024-y2020)/y2020*100:+.1f}%)\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Q8W6TEc8IlhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영어: 인문학, 사회과학"
      ],
      "metadata": {
        "id": "FBDEZgU6k5mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한글 폰트 설정 (Windows의 경우)\n",
        "# plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "# 1. 과잉 단어 파일 경로\n",
        "# 필요한 파일: (H 혹은 SS)_excess_words_top_labeled.xlsx\n",
        "EXCESS_WORDS_FILE = \"/content/SS_excess_words_top_labeled.xlsx\"\n",
        "\n",
        "# 2. 원본 초록 데이터 파일 경로 (전처리된 CSV/Excel)\n",
        "# 필요한 파일: (H 혹은 SS)_eng_abstract_clean.csv\n",
        "ABSTRACTS_FILE = \"/content/SS_eng_abstract_clean.csv\"\n",
        "\n",
        "# 3. 출력된 폴더\n",
        "OUTPUT_DIR = Path(\"/content/excess_analysis\")\n",
        "\n",
        "# 4. 분석 기준 (과잉 단어 N개 이상)\n",
        "THRESHOLDS = [1, 3, 5, 10, 15, 20]\n",
        "\n",
        "# 5. 분야명 (그래프 제목용)\n",
        "FIELD_NAME = \"Humanities\"  # 또는 \"Social Sciences\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 함수 정의\n",
        "# ============================================================\n",
        "\n",
        "def load_excess_words(filepath):\n",
        "    \"\"\"과잉 단어 목록 로드\"\"\"\n",
        "    df = pd.read_excel(filepath)\n",
        "    words = set(df['word'].tolist())\n",
        "    print(f\"과잉 단어 {len(words)}개 로드 완료\")\n",
        "    return words, df\n",
        "\n",
        "\n",
        "def load_abstracts(filepath, encoding='utf-8-sig'):\n",
        "    \"\"\"초록 데이터 로드\"\"\"\n",
        "    if filepath.endswith('.xlsx') or filepath.endswith('.xls'):\n",
        "        df = pd.read_excel(filepath)\n",
        "    else:\n",
        "        df = pd.read_csv(filepath, encoding=encoding)\n",
        "    print(f\"초록 데이터 {len(df)}건 로드 완료\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def count_excess_in_text(text, excess_words):\n",
        "    \"\"\"\n",
        "    텍스트에서 과잉 단어 출현 횟수 계산 (단순 문자열 매칭)\n",
        "\n",
        "    Returns:\n",
        "        - count: 과잉 단어 총 출현 횟수 (중복 포함)\n",
        "        - unique_count: 사용된 고유 과잉 단어 개수\n",
        "        - found_words: 발견된 과잉 단어 리스트\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return 0, 0, []\n",
        "\n",
        "    found_words = []\n",
        "    for word in excess_words:\n",
        "        cnt = text.count(word)\n",
        "        if cnt > 0:\n",
        "            found_words.extend([word] * cnt)\n",
        "\n",
        "    count = len(found_words)\n",
        "    unique_count = len(set(found_words))\n",
        "\n",
        "    return count, unique_count, list(set(found_words))\n",
        "\n",
        "\n",
        "def analyze_all_abstracts(df, excess_words, year_col='발행년도', abstract_col='영어초록'):\n",
        "    \"\"\"전체 초록 데이터 분석\"\"\"\n",
        "    print(\"\\n초록 분석 중...\")\n",
        "\n",
        "    results = []\n",
        "    total = len(df)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if idx % 10000 == 0:\n",
        "            print(f\"  진행: {idx:,} / {total:,} ({idx/total*100:.1f}%)\")\n",
        "\n",
        "        year = row[year_col]\n",
        "        text = row[abstract_col]\n",
        "\n",
        "        count, unique_count, found = count_excess_in_text(text, excess_words)\n",
        "\n",
        "        # 텍스트 길이 (대략적인 어절 수)\n",
        "        text_len = len(str(text).split()) if pd.notna(text) else 0\n",
        "        ratio = count / text_len if text_len > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            'year': year,\n",
        "            'excess_count': count,\n",
        "            'excess_unique': unique_count,\n",
        "            'excess_ratio': ratio,\n",
        "            'text_length': text_len\n",
        "        })\n",
        "\n",
        "    print(f\"  완료: {total:,}건 분석\")\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def calculate_threshold_stats(analysis_df, thresholds):\n",
        "    \"\"\"방법 2: N개 이상 과잉 단어 사용한 초록 비율 계산\"\"\"\n",
        "    years = sorted(analysis_df['year'].unique())\n",
        "\n",
        "    stats = []\n",
        "    for year in years:\n",
        "        year_data = analysis_df[analysis_df['year'] == year]\n",
        "        total = len(year_data)\n",
        "\n",
        "        row = {'year': year, 'total_abstracts': total}\n",
        "\n",
        "        for n in thresholds:\n",
        "            count = (year_data['excess_unique'] >= n).sum()\n",
        "            ratio = count / total * 100 if total > 0 else 0\n",
        "            row[f'n>={n}_count'] = count\n",
        "            row[f'n>={n}_ratio'] = ratio\n",
        "\n",
        "        stats.append(row)\n",
        "\n",
        "    return pd.DataFrame(stats)\n",
        "\n",
        "\n",
        "def plot_threshold_trends(stats_df, thresholds, output_path, field_name=\"\"):\n",
        "    \"\"\"방법 2 시각화: N개 이상 사용 비율 추이\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    years = stats_df['year'].values\n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(thresholds)))\n",
        "\n",
        "    for i, n in enumerate(thresholds):\n",
        "        col = f'n>={n}_ratio'\n",
        "        ax.plot(years, stats_df[col], marker='o', label=f'{n}+ words',\n",
        "                color=colors[i], linewidth=2, markersize=5)\n",
        "\n",
        "    ax.set_xlabel('Year', fontsize=12)\n",
        "    ax.set_ylabel('Percentage of Abstracts (%)', fontsize=12)\n",
        "    ax.set_title(f'[{field_name}] Abstracts Using N+ Excess Words Over Time', fontsize=14)\n",
        "    ax.legend(loc='upper left', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xticks(years[::2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"저장: {output_path}\")\n",
        "\n",
        "\n",
        "def plot_score_distribution(analysis_df, output_path, field_name=\"\"):\n",
        "    \"\"\"방법 3 시각화: 연도별 과잉 점수 분포 (박스플롯)\"\"\"\n",
        "    years = sorted(analysis_df['year'].unique())\n",
        "    recent_years = years[-10:] if len(years) > 10 else years\n",
        "    recent_data = analysis_df[analysis_df['year'].isin(recent_years)]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # (1) 고유 과잉 단어 개수 분포\n",
        "    ax1 = axes[0]\n",
        "    data_unique = [recent_data[recent_data['year'] == y]['excess_unique'].values\n",
        "                   for y in recent_years]\n",
        "    bp1 = ax1.boxplot(data_unique, labels=recent_years, patch_artist=True)\n",
        "    for patch in bp1['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "    ax1.set_xlabel('Year', fontsize=11)\n",
        "    ax1.set_ylabel('Unique Excess Words per Abstract', fontsize=11)\n",
        "    ax1.set_title(f'[{field_name}] Distribution of Excess Word Count', fontsize=12)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # (2) 과잉 단어 비율 분포\n",
        "    ax2 = axes[1]\n",
        "    data_ratio = [recent_data[recent_data['year'] == y]['excess_ratio'].values * 100\n",
        "                  for y in recent_years]\n",
        "    bp2 = ax2.boxplot(data_ratio, labels=recent_years, patch_artist=True)\n",
        "    for patch in bp2['boxes']:\n",
        "        patch.set_facecolor('lightcoral')\n",
        "    ax2.set_xlabel('Year', fontsize=11)\n",
        "    ax2.set_ylabel('Excess Word Ratio (%)', fontsize=11)\n",
        "    ax2.set_title(f'[{field_name}] Distribution of Excess Word Ratio', fontsize=12)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"저장: {output_path}\")\n",
        "\n",
        "\n",
        "def plot_yearly_mean_trend(analysis_df, output_path, field_name=\"\"):\n",
        "    \"\"\"연도별 평균 과잉 단어 사용량 추이\"\"\"\n",
        "    yearly_stats = analysis_df.groupby('year').agg({\n",
        "        'excess_count': ['mean', 'std', 'median'],\n",
        "        'excess_unique': ['mean', 'std', 'median'],\n",
        "        'excess_ratio': ['mean', 'std', 'median']\n",
        "    }).reset_index()\n",
        "\n",
        "    yearly_stats.columns = ['year',\n",
        "                            'count_mean', 'count_std', 'count_median',\n",
        "                            'unique_mean', 'unique_std', 'unique_median',\n",
        "                            'ratio_mean', 'ratio_std', 'ratio_median']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    years = yearly_stats['year'].values\n",
        "\n",
        " # (1) 평균 고유 과잉 단어 개수\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(years, yearly_stats['unique_mean'], 'b-o', linewidth=2,\n",
        "             markersize=6, label='Mean')\n",
        "    ax1.plot(years, yearly_stats['unique_median'], 'b--s', linewidth=1.5,\n",
        "             markersize=4, alpha=0.7, label='Median')\n",
        "\n",
        "    ax1.set_xlabel('Year', fontsize=11)\n",
        "    ax1.set_ylabel('Unique Excess Words', fontsize=11)\n",
        "    ax1.set_title(f'[{field_name}] Mean Excess Word Count per Abstract', fontsize=12)\n",
        "    ax1.legend(fontsize=9)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    n_samples = analysis_df.groupby('year').size().values\n",
        "    ci_unique = 1.96 * yearly_stats['unique_std'] / np.sqrt(n_samples)\n",
        "    ax1.fill_between(years,\n",
        "                 yearly_stats['unique_mean'] - ci_unique,\n",
        "                 yearly_stats['unique_mean'] + ci_unique,\n",
        "                 alpha=0.2)\n",
        "    ax1.set_xticks(years[::2])\n",
        "    # (2) 평균 과잉 단어 비율\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(years, yearly_stats['ratio_mean'] * 100, 'r-o', linewidth=2,\n",
        "             markersize=6, label='Mean')\n",
        "    ax2.plot(years, yearly_stats['ratio_median'] * 100, 'r--s', linewidth=1.5,\n",
        "             markersize=4, alpha=0.7, label='Median')\n",
        "\n",
        "    ax2.set_xlabel('Year', fontsize=11)\n",
        "    ax2.set_ylabel('Excess Word Ratio (%)', fontsize=11)\n",
        "    ax2.set_title(f'[{field_name}] Mean Excess Word Ratio per Abstract', fontsize=12)\n",
        "    ax2.legend(fontsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    ci_ratio = 1.96 * yearly_stats['ratio_std'] / np.sqrt(n_samples)\n",
        "    ax2.fill_between(years,\n",
        "                 (yearly_stats['ratio_mean'] - ci_ratio) * 100,\n",
        "                 (yearly_stats['ratio_mean'] + ci_ratio) * 100,\n",
        "                 alpha=0.2, color='red')\n",
        "    ax2.set_xticks(years[::2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"저장: {output_path}\")\n",
        "\n",
        "    return yearly_stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"과잉 단어 사용 추이 분석\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 출력 폴더 생성\n",
        "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1. 과잉 단어 로드\n",
        "    print(\"\\n[1] 과잉 단어 로드 중...\")\n",
        "    excess_words, excess_df = load_excess_words(EXCESS_WORDS_FILE)\n",
        "\n",
        "    # 2. 초록 데이터 로드\n",
        "    print(\"\\n[2] 초록 데이터 로드 중...\")\n",
        "    abstracts_df = load_abstracts(ABSTRACTS_FILE)\n",
        "\n",
        "    # 3. 분석 실행\n",
        "    print(\"\\n[3] 초록별 과잉 단어 분석...\")\n",
        "    analysis_df = analyze_all_abstracts(abstracts_df, excess_words)\n",
        "\n",
        "    # 분석 결과 저장\n",
        "    analysis_df.to_csv(OUTPUT_DIR / 'abstract_excess_analysis.csv',\n",
        "                       index=False, encoding='utf-8-sig')\n",
        "    print(f\"  → 저장: {OUTPUT_DIR / 'abstract_excess_analysis.csv'}\")\n",
        "\n",
        "    # 4. 방법 2: N개 이상 사용 비율\n",
        "    print(\"\\n[4] 임계값별 비율 계산...\")\n",
        "    stats_df = calculate_threshold_stats(analysis_df, THRESHOLDS)\n",
        "    stats_df.to_csv(OUTPUT_DIR / 'threshold_stats.csv',\n",
        "                    index=False, encoding='utf-8-sig')\n",
        "    print(stats_df.to_string())\n",
        "\n",
        "    # 5. 시각화\n",
        "    print(\"\\n[5] 시각화...\")\n",
        "    plot_threshold_trends(stats_df, THRESHOLDS,\n",
        "                          OUTPUT_DIR / 'threshold_trends.png', FIELD_NAME)\n",
        "    plot_score_distribution(analysis_df,\n",
        "                            OUTPUT_DIR / 'score_distribution.png', FIELD_NAME)\n",
        "    yearly_stats = plot_yearly_mean_trend(analysis_df,\n",
        "                                          OUTPUT_DIR / 'yearly_mean_trend.png', FIELD_NAME)\n",
        "\n",
        "    # 연도별 통계 저장\n",
        "    yearly_stats.to_csv(OUTPUT_DIR / 'yearly_stats.csv',\n",
        "                        index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 분석 완료!\")\n",
        "    print(f\"   결과 저장 위치: {OUTPUT_DIR}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 요약 통계 출력\n",
        "    print(\"\\n[요약]\")\n",
        "    print(f\"- 전체 초록 수: {len(analysis_df):,}\")\n",
        "    print(f\"- 과잉 단어 1개 이상 사용한 초록: {(analysis_df['excess_unique'] >= 1).sum():,} \"\n",
        "          f\"({(analysis_df['excess_unique'] >= 1).mean()*100:.1f}%)\")\n",
        "    print(f\"- 과잉 단어 5개 이상 사용한 초록: {(analysis_df['excess_unique'] >= 5).sum():,} \"\n",
        "          f\"({(analysis_df['excess_unique'] >= 5).mean()*100:.1f}%)\")\n",
        "    print(f\"- 평균 과잉 단어 개수: {analysis_df['excess_unique'].mean():.2f}\")\n",
        "\n",
        "    # 2024년 vs 2020년 비교\n",
        "    if 2024 in analysis_df['year'].values and 2020 in analysis_df['year'].values:\n",
        "        y2020 = analysis_df[analysis_df['year'] == 2020]['excess_unique'].mean()\n",
        "        y2024 = analysis_df[analysis_df['year'] == 2024]['excess_unique'].mean()\n",
        "        print(f\"\\n- 2020년 평균: {y2020:.2f} → 2024년 평균: {y2024:.2f} \"\n",
        "              f\"(변화: {(y2024-y2020)/y2020*100:+.1f}%)\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOFMN3-CprrH",
        "outputId": "aa2ef2e2-942c-4ead-e3eb-4f1bdc8b0e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "과잉 단어 사용 추이 분석\n",
            "============================================================\n",
            "\n",
            "[1] 과잉 단어 로드 중...\n",
            "과잉 단어 196개 로드 완료\n",
            "\n",
            "[2] 초록 데이터 로드 중...\n",
            "초록 데이터 511787건 로드 완료\n",
            "\n",
            "[3] 초록별 과잉 단어 분석...\n",
            "\n",
            "초록 분석 중...\n",
            "  진행: 0 / 511,787 (0.0%)\n",
            "  진행: 10,000 / 511,787 (2.0%)\n",
            "  진행: 20,000 / 511,787 (3.9%)\n",
            "  진행: 30,000 / 511,787 (5.9%)\n",
            "  진행: 40,000 / 511,787 (7.8%)\n",
            "  진행: 50,000 / 511,787 (9.8%)\n",
            "  진행: 60,000 / 511,787 (11.7%)\n",
            "  진행: 70,000 / 511,787 (13.7%)\n",
            "  진행: 80,000 / 511,787 (15.6%)\n",
            "  진행: 90,000 / 511,787 (17.6%)\n",
            "  진행: 100,000 / 511,787 (19.5%)\n",
            "  진행: 110,000 / 511,787 (21.5%)\n",
            "  진행: 120,000 / 511,787 (23.4%)\n",
            "  진행: 130,000 / 511,787 (25.4%)\n",
            "  진행: 140,000 / 511,787 (27.4%)\n",
            "  진행: 150,000 / 511,787 (29.3%)\n",
            "  진행: 160,000 / 511,787 (31.3%)\n",
            "  진행: 170,000 / 511,787 (33.2%)\n",
            "  진행: 180,000 / 511,787 (35.2%)\n",
            "  진행: 190,000 / 511,787 (37.1%)\n",
            "  진행: 200,000 / 511,787 (39.1%)\n",
            "  진행: 210,000 / 511,787 (41.0%)\n",
            "  진행: 220,000 / 511,787 (43.0%)\n",
            "  진행: 230,000 / 511,787 (44.9%)\n",
            "  진행: 240,000 / 511,787 (46.9%)\n",
            "  진행: 250,000 / 511,787 (48.8%)\n",
            "  진행: 260,000 / 511,787 (50.8%)\n",
            "  진행: 270,000 / 511,787 (52.8%)\n",
            "  진행: 280,000 / 511,787 (54.7%)\n",
            "  진행: 290,000 / 511,787 (56.7%)\n",
            "  진행: 300,000 / 511,787 (58.6%)\n",
            "  진행: 310,000 / 511,787 (60.6%)\n",
            "  진행: 320,000 / 511,787 (62.5%)\n",
            "  진행: 330,000 / 511,787 (64.5%)\n",
            "  진행: 340,000 / 511,787 (66.4%)\n",
            "  진행: 350,000 / 511,787 (68.4%)\n",
            "  진행: 360,000 / 511,787 (70.3%)\n",
            "  진행: 370,000 / 511,787 (72.3%)\n",
            "  진행: 380,000 / 511,787 (74.2%)\n",
            "  진행: 390,000 / 511,787 (76.2%)\n",
            "  진행: 400,000 / 511,787 (78.2%)\n",
            "  진행: 410,000 / 511,787 (80.1%)\n",
            "  진행: 420,000 / 511,787 (82.1%)\n",
            "  진행: 430,000 / 511,787 (84.0%)\n",
            "  진행: 440,000 / 511,787 (86.0%)\n",
            "  진행: 450,000 / 511,787 (87.9%)\n",
            "  진행: 460,000 / 511,787 (89.9%)\n",
            "  진행: 470,000 / 511,787 (91.8%)\n",
            "  진행: 480,000 / 511,787 (93.8%)\n",
            "  진행: 490,000 / 511,787 (95.7%)\n",
            "  진행: 500,000 / 511,787 (97.7%)\n",
            "  진행: 510,000 / 511,787 (99.7%)\n",
            "  완료: 511,787건 분석\n",
            "  → 저장: /content/excess_analysis/abstract_excess_analysis.csv\n",
            "\n",
            "[4] 임계값별 비율 계산...\n",
            "    year  total_abstracts  n>=1_count  n>=1_ratio  n>=3_count  n>=3_ratio  n>=5_count  n>=5_ratio  n>=10_count  n>=10_ratio  n>=15_count  n>=15_ratio  n>=20_count  n>=20_ratio\n",
            "0   2004             9017        8937   99.112787        8551   94.831984        7783   86.314739         3245    35.987579          530     5.877786           56     0.621049\n",
            "1   2005            10612       10533   99.255560       10114   95.307199        9154   86.260837         3872    36.486996          633     5.964945           86     0.810403\n",
            "2   2006            11906       11432   96.018814       10334   86.796573        9325   78.321855         4162    34.957164          759     6.374937          102     0.856711\n",
            "3   2007            14594       14254   97.670275       13397   91.797999       12199   83.589146         5670    38.851583         1090     7.468823          146     1.000411\n",
            "4   2008            18361       18292   99.624203       17664   96.203910       16385   89.238059         8123    44.240510         1843    10.037580          342     1.862644\n",
            "5   2009            21362       21277   99.602097       20517   96.044378       19085   89.340886         9719    45.496676         2336    10.935306          477     2.232937\n",
            "6   2010            23097       23024   99.683942       22347   96.752825       20973   90.804001        11041    47.802745         2699    11.685500          586     2.537126\n",
            "7   2011            24412       24313   99.594462       23677   96.989186       22264   91.201049        11851    48.545797         2899    11.875307          599     2.453711\n",
            "8   2012            25332       25271   99.759198       24640   97.268277       23308   92.010106        12694    50.110532         3214    12.687510          631     2.490921\n",
            "9   2013            26071       26016   99.789038       25402   97.433930       24064   92.301791        13341    51.171800         3319    12.730620          643     2.466342\n",
            "10  2014            27380       27315   99.762600       26787   97.834186       25516   93.192111        14453    52.786706         3730    13.623083          686     2.505478\n",
            "11  2015            28336       28190   99.484754       27723   97.836674       26490   93.485319        15211    53.680830         4054    14.306889          706     2.491530\n",
            "12  2016            28359       28209   99.471067       27692   97.648013       26515   93.497655        15515    54.709263         4110    14.492754          755     2.662294\n",
            "13  2017            28643       28437   99.280802       28029   97.856370       26810   93.600531        15333    53.531404         3859    13.472751          542     1.892260\n",
            "14  2018            29541       29399   99.519312       28993   98.144951       27783   94.048949        15782    53.424055         3786    12.816086          544     1.841508\n",
            "15  2019            30236       30101   99.553512       29723   98.303347       28538   94.384178        16479    54.501257         4154    13.738590          626     2.070380\n",
            "16  2020            31702       31652   99.842281       31343   98.867579       30253   95.429310        18499    58.352785         5008    15.797111          716     2.258533\n",
            "17  2021            32253       32174   99.755062       31871   98.815614       30867   95.702725        18929    58.689114         5009    15.530338          724     2.244752\n",
            "18  2022            30563       30500   99.793868       30240   98.943167       29203   95.550175        17250    56.440794         4355    14.249256          636     2.080948\n",
            "19  2023            29835       29774   99.795542       29550   99.044746       28788   96.490699        19593    65.671192         7672    25.714765         2551     8.550360\n",
            "20  2024            30175       30160   99.950290       29963   99.297432       29492   97.736537        23569    78.107705        13376    44.328086         6391    21.179785\n",
            "\n",
            "[5] 시각화...\n",
            "저장: /content/excess_analysis/threshold_trends.png\n",
            "저장: /content/excess_analysis/score_distribution.png\n",
            "저장: /content/excess_analysis/yearly_mean_trend.png\n",
            "\n",
            "============================================================\n",
            "✅ 분석 완료!\n",
            "   결과 저장 위치: /content/excess_analysis\n",
            "============================================================\n",
            "\n",
            "[요약]\n",
            "- 전체 초록 수: 511,787\n",
            "- 과잉 단어 1개 이상 사용한 초록: 509,260 (99.5%)\n",
            "- 과잉 단어 5개 이상 사용한 초록: 474,795 (92.8%)\n",
            "- 평균 과잉 단어 개수: 10.33\n",
            "\n",
            "- 2020년 평균: 10.61 → 2024년 평균: 14.68 (변화: +38.4%)\n"
          ]
        }
      ]
    }
  ]
}